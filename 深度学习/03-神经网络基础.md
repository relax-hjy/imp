神经网络的介绍

![image-20250206145120314](./assets/image-20250206145120314.png)

神经元：输入和权重的乘积求和（加权和）加上非线性激活函数，意味着线、面、超平面向曲线，曲面、超曲面的转化。

![image-20250206145409248](./assets/image-20250206145409248.png)

神经网络就是神经元的堆叠。

神经网络的一般结构。输入层+隐藏层+输出层

输入层：输入数据

隐藏层：提取特征，图中提取了两次特征。

输出层：输出最终结果

特点是：

同一层的神经元之间是没有连接的。

第N层的每个神经元和第N-1层的所有神经元相连（full connected 全连接）。

第N-1层的输出就是第N层的输入。

每个连接都有一个w权重



激活函数：

激活函数用于对每层的输出数据进行变换，进而为整个网络注入了非线性因素。此时可以拟合各种曲线曲面。

不引入非线性，神经元的堆叠没有意义。线性组合的线性运算可以话简成一个线性组合。

引入非线性，可以通过神经元的堆叠逼近任意函数，提升网络对复杂问题，的拟合能力。



sigmoid：

![image-20250206150900892](./assets/image-20250206150900892.png)

f(x)=1/(1+e-x)    f`(x)=f(x)(1-f(x))



tanh()

![image-20250206152304426](./assets/image-20250206152304426.png)

ReLU激活函数

![image-20250206152431442](./assets/image-20250206152431442.png)

1.relu死区问题，神经元的死亡问题，线性组合的结果如果小于零，那么神经元死亡。

2.计算量小。

3.一旦一个神经元的输出为零，它在反向传播中接收到的梯度也是零。

![a41921e7a65e78712b0f26eb301646a](./assets/a41921e7a65e78712b0f26eb301646a.jpg)

![image-20250206152807652](./assets/image-20250206152807652.png)

结果由同一层相互作用。



使用时

隐藏层优先用relu，可以尝试leakyrelu

避免大量神经元死亡的情况

少用sigmoid激活函数，可以尝试tanh激活函数



输出层

二分类用sigmoid

多分类用softmax

回归问题用identity



初始化方法：

均匀，正态，凯明，哈维



损失函数：

```
# 损失函数描述模型输出和真实值的差异
```

多分类交叉熵损失

![image-20250207140543810](./assets/image-20250207140543810.png)

![image-20250207141032844](./assets/image-20250207141032844.png)

二分类损失

![image-20250207141633137](./assets/image-20250207141633137.png)

MAE损失

![image-20250207142047180](./assets/image-20250207142047180.png)

![image-20250207142223407](./assets/image-20250207142223407.png)

L2 MSE损失

![image-20250207142826405](./assets/image-20250207142826405.png)

smoothL1

![image-20250207143126735](./assets/image-20250207143126735.png)





优化方法



![image-20250207175045994](./assets/image-20250207175045994.png)



# Momentum
# 用指数加权平均法来更新梯度



adagrad

![image-20250207181553763](./assets/image-20250207181553763.png)



