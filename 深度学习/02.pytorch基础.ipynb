{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "本案例学习pytorch相关操作\n",
    "张量的基本属性  dtype  grad \n",
    "1.张量创建\n",
    "    基本创建\n",
    "    torch.tensor（） torch.Tensor()  根据数据创建张量  根据形状创建张量\n",
    "    torch.IntTensor() 根据形状创建张量时指定类型\n",
    "    线性张量和随机张量\n",
    "    torch.arange() torch.linspace() 创建线性张量\n",
    "    torch.randn()  torch.rand()\n",
    "    torch.initial_seed()  torch.manual_seed() 查看和设置随机种子\n",
    "    创建0，1张量和指定值张量\n",
    "    torch.zeros() torch.ones()\n",
    "    torch.zeros_like() torch.ones_like()\n",
    "    torch.full() torch.full_like()\n",
    "    张量元素类型转换\n",
    "    .type()  .float() .to()\n",
    "    张量的类型转换\n",
    "    tensor.numpy() tensor.numpy().copy()\n",
    "    torch.from_numpy(ndarray) torch.from_numpy(ndarray.copy()) torch.Tensor()\n",
    "    torch.item() torch.torch()\n",
    "2.张量的运算\n",
    "    基本运算\n",
    "    数学计算\n",
    "3.张量的索引操作\n",
    "\n",
    "    \"\"\""
   ],
   "id": "5a1d35881e0a4132"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T18:04:47.093742Z",
     "start_time": "2025-06-27T18:04:43.495923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 致昨天，期待再见\n",
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "from loguru import logger\n",
    "def version_info():\n",
    "    logger.info(\"本项目是pytorch基础\")\n",
    "    logger.info(f\"系统版本:{platform.platform()}\")\n",
    "    logger.info(f\"python版本:{sys.version,sys.version_info,platform.python_version()}\")\n",
    "    logger.info(f\"pytorch版本:{torch.__version__}\")# 查看version版本      python3.8-3.11 支持2.0.0\n",
    "version_info()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-06-28 02:04:47.073\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mversion_info\u001B[0m:\u001B[36m7\u001B[0m - \u001B[1m本项目是pytorch基础\u001B[0m\n",
      "\u001B[32m2025-06-28 02:04:47.075\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mversion_info\u001B[0m:\u001B[36m8\u001B[0m - \u001B[1m系统版本:Windows-10-10.0.26100-SP0\u001B[0m\n",
      "\u001B[32m2025-06-28 02:04:47.076\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mversion_info\u001B[0m:\u001B[36m9\u001B[0m - \u001B[1mpython版本:('3.10.0 | packaged by conda-forge | (default, Nov 10 2021, 13:20:59) [MSC v.1916 64 bit (AMD64)]', sys.version_info(major=3, minor=10, micro=0, releaselevel='final', serial=0), '3.10.0')\u001B[0m\n",
      "\u001B[32m2025-06-28 02:04:47.076\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mversion_info\u001B[0m:\u001B[36m10\u001B[0m - \u001B[1mpytorch版本:2.4.0+cu124\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:54:11.983168Z",
     "start_time": "2025-01-23T05:54:11.976449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 深度学习基础\n",
    "      # 张量创建   # 张量的转化 # 张量的运算  # 张量的索引 # 张量的形状操作\n",
    "# 2. Pytorch的基本使用\n",
    "# 3. 神经网络基础\n",
    "# 4. 卷积神经网络\n",
    "# 5. 循环神经网络\n",
    "# 6. transformer"
   ],
   "id": "9c8e6efc72777feb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T05:54:13.787490Z",
     "start_time": "2025-01-23T05:54:13.772415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 张量 Tensor ，机器学习的基本构建模块，能保存梯度\n",
    "# 在图像处理领域，一般用三维的张量表示三通道，高，宽表示图像\n",
    "# "
   ],
   "id": "6309753c7e26edb1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:12:22.007952Z",
     "start_time": "2025-06-26T10:12:21.991953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 张量的维度（ndim）和方括号层数对应，size()的方括号内就有几个数。张量的维度可以从0到无限多。\n",
    "scalar=torch.tensor(7) # 源数据没有方括号，0维，\n",
    "print(f\"0维度的张量{scalar}\",scalar,scalar.ndim,scalar.size())\n",
    "vector=torch.tensor([1,2,3]) # 源数据只有一个方括号，1维，\n",
    "print(f\"1维度的张量{vector}\",vector.ndim,vector.size())\n",
    "matrix=torch.tensor([[1,2,3],[4,5,6]])  # 源数据二个方括号，2维\n",
    "print(f\"2维度的张量{matrix}\",matrix.ndim,vector.size())\n",
    "tensor=torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n",
    "print(f\"多维度的张量{tensor}\",tensor.ndim,tensor.size()) # 两通道，两行三列"
   ],
   "id": "d9156426f43c9568",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0维度的张量7 tensor(7) 0 torch.Size([])\n",
      "1维度的张量tensor([1, 2, 3]) 1 torch.Size([3])\n",
      "2维度的张量tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]) 2 torch.Size([3])\n",
      "多维度的张量tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]]) 3 torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T03:32:58.331465Z",
     "start_time": "2025-02-05T03:32:58.283300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tensor()和Tensor()的区别  tensor()方法只能通过已有数据，如标量，数组，列表创建张量，Tensor()即可以通过数组和列表创建张量，也可以通过形状创建张量，注意Tensor(3)没有方括号被视为用形状创建。\n",
    "import numpy as np\n",
    "a=torch.tensor(2) # 根据数据创建\n",
    "b=torch.Tensor(3) # 根据数据或形状，没有方括号的情况下都是指定形状\n",
    "print(a,b)\n",
    "\n",
    "# 由numpy创建数组，再通过数组创建tensor\n",
    "np1=np.random.randn(2,3)\n",
    "ts1=torch.tensor(np1)   # 数组创建\n",
    "ts2=torch.tensor([[1,2,3],[2,3,4]])  # 列表创建\n",
    "print(ts1)\n",
    "\n",
    "ts23=torch.Tensor(2,3)\n",
    "print(ts23,ts23.dtype)  # 默认是float\n",
    "\n",
    "# 指定类型\n",
    "tsint=torch.IntTensor(2,3)  # 等于 torch.Tensor(2,3,dtype=Int)\n",
    "print(tsint,tsint.dtype)\n",
    "tsDouble=torch.DoubleTensor(2,3)\n",
    "print(tsDouble,tsDouble.dtype)"
   ],
   "id": "4d88a12ae2fb06c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2) tensor([0., 0., 0.])\n",
      "tensor([[ 0.2503, -0.3940,  1.1754],\n",
      "        [-0.0426, -0.5220,  0.5532]], dtype=torch.float64)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 7.2447e-43]]) torch.float32\n",
      "tensor([[        0,         0,         0],\n",
      "        [        0,         0, 539767391]], dtype=torch.int32) torch.int32\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64) torch.float64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T08:37:42.198030Z",
     "start_time": "2025-06-27T08:37:41.363051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建线性和随机张量\n",
    "# 线性张量\n",
    "ts_246=torch.arange(0,10,2)#[start,end,step]左闭右开,step步长\n",
    "print(ts_246)\n",
    "ts_lin=torch.linspace(0,11,10)# [start,end,nums]左闭右闭，在区间内均匀地取多少个数\n",
    "print(ts_lin)\n",
    "\n",
    "# 设置和查看随机种子，torch.randn()生成固定张量 ，rand() ,randint()\n",
    "# 查看随机种子\n",
    "print(torch.random.initial_seed())\n",
    "# 设置随机种子\n",
    "print(torch.random.manual_seed(100))\n",
    "# 创建满足正态分布的随即张量\n",
    "print(torch.randn(2, 3))\n",
    "# torch.Tensor(2,3)\n",
    "print(torch.random.initial_seed())\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# torch.cuda.manual_seed(42)\n",
    "# 在 DataLoader 中设置参数 worker_init_fn 来设置每个 worker 的种子；\n",
    "# 开启 PyTorch 的确定性算法（注意这可能会影响性能）：\n",
    "# torch.use_deterministic_algorithms(True)"
   ],
   "id": "edd723eb175c21d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([ 0.0000,  1.2222,  2.4444,  3.6667,  4.8889,  6.1111,  7.3333,  8.5556,\n",
      "         9.7778, 11.0000])\n",
      "284525191834800\n",
      "<torch._C.Generator object at 0x000002EC0E0E8A50>\n",
      "tensor([[ 0.3607, -0.2859, -0.3938],\n",
      "        [ 0.2429, -1.3833, -2.3134]])\n",
      "100\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T08:00:34.371904Z",
     "start_time": "2025-01-20T08:00:34.352347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建01张量   zeros,ones,zeros_like,ones_like\n",
    "ts_0=torch.zeros(2,3)\n",
    "print(ts_0)\n",
    "ts_1=torch.ones(2,3)\n",
    "print(ts_1)\n",
    "ts_rand=torch.rand(3,3)\n",
    "print(ts_rand)\n",
    "ts_randn=torch.randn(3,3)\n",
    "print(ts_randn)\n",
    "ts_zeros_like=torch.zeros_like(ts_rand)\n",
    "print(ts_zeros_like)"
   ],
   "id": "a553ecf01148bd03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.6868, 0.4920, 0.0748],\n",
      "        [0.9605, 0.3271, 0.0103],\n",
      "        [0.9516, 0.2855, 0.2324]])\n",
      "tensor([[-0.3420,  0.8223,  0.2766],\n",
      "        [-0.9687, -0.7782,  0.5023],\n",
      "        [ 0.5022,  0.8530, -0.4760]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T08:02:25.999691Z",
     "start_time": "2025-01-20T08:02:25.979442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# full,full_like\n",
    "# 创建指定值的张量\n",
    "ts_10=torch.full([2,3],10) # 左形状，有指定值\n",
    "print(ts_10)\n",
    "ts_rand=torch.rand(3,3)\n",
    "ts_10_like=torch.full_like(ts_rand,10)\n",
    "print(ts_10_like)"
   ],
   "id": "51f41341d37cfe96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 10, 10],\n",
      "        [10, 10, 10]])\n",
      "tensor([[10., 10., 10.],\n",
      "        [10., 10., 10.],\n",
      "        [10., 10., 10.]])\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T08:04:44.048674Z",
     "start_time": "2025-01-20T08:04:44.024820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 张量元素类型转换,short in16 int int32 long int64 float float32 double float64\n",
    "#  type(),to(),float()\n",
    "ts_int=torch.tensor([1,2,3],dtype=torch.int)\n",
    "print(ts_int)\n",
    "ts_float=ts_int.float()\n",
    "print(ts_float)\n",
    "ts_float2=ts_int.to(torch.double)\n",
    "print(ts_float2)\n",
    "ts_float3=ts_int.type(torch.DoubleTensor)\n",
    "print(ts_float3)"
   ],
   "id": "d95c6e42dc8caba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1., 2., 3.], dtype=torch.float64)\n",
      "tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T09:20:06.890735Z",
     "start_time": "2025-06-27T09:20:06.867095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 张量与数组的转换\n",
    "data_tensor=torch.tensor([2,3,4])\n",
    "print(type(data_tensor))\n",
    "\n",
    "# 共享内存的转换\n",
    "data_numpy=data_tensor.numpy()\n",
    "print(type(data_numpy))\n",
    "print(data_tensor,data_numpy)\n",
    "data_numpy[0]=100  \n",
    "print(data_tensor,data_numpy)\n",
    "\n",
    "# 用深拷贝不共享内存\n",
    "data_numpy=data_tensor.numpy().copy()   # 这里的copy是深拷贝\n",
    "print(type(data_numpy))\n",
    "data_numpy[0]=200\n",
    "print(data_tensor,data_numpy)"
   ],
   "id": "fba663ac8d40f745",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([2, 3, 4]) [2 3 4]\n",
      "tensor([100,   3,   4]) [100   3   4]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([100,   3,   4]) [200   3   4]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T08:49:07.330641Z",
     "start_time": "2025-01-20T08:49:07.306627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_np=np.array([2,3,4])\n",
    "data_ts=torch.from_numpy(data_np)\n",
    "data_ts[0]=100\n",
    "print(data_np,data_ts)\n",
    "\n",
    "torch.tensor(data_np) # 本身不共享内存\n",
    "data_ts=torch.from_numpy(data_np.copy())"
   ],
   "id": "534a8ff5d74687e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100   3   4] tensor([100,   3,   4], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([100,   3,   4], dtype=torch.int32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T18:20:36.649163Z",
     "start_time": "2025-06-27T18:20:36.634144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 用item()这个函数将只有一个元素的张量提取出来\n",
    "data=torch.Tensor([30])\n",
    "print(data.item())\n",
    "\n",
    "data=torch.tensor(30)\n",
    "print(data.item())\n",
    "print(type(data.item()))"
   ],
   "id": "8eb96d2bce60dfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "30\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 张量基本运算符号",
   "id": "9fa56581d91b4a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T06:06:48.838536Z",
     "start_time": "2025-02-05T06:06:48.825018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data=torch.randint(0,10,[2,3]) # 左闭右开\n",
    "print(data)\n",
    "new=data.add(10)\n",
    "print(new)\n",
    "print(data)\n",
    "# sub() mul() div() neg()"
   ],
   "id": "8849f7d081ddfb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 7, 0],\n",
      "        [6, 7, 9]])\n",
      "tensor([[13, 17, 10],\n",
      "        [16, 17, 19]])\n",
      "tensor([[3, 7, 0],\n",
      "        [6, 7, 9]])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T08:59:06.018979Z",
     "start_time": "2025-01-20T08:59:05.997805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 对应元素相乘\n",
    "data1=torch.tensor([[1,2],[3,4]])\n",
    "data2=torch.tensor([[5,6],[7,8]])\n",
    "data3=data1.mul(data2)\n",
    "data4=data1*data2\n",
    "print(data3,data4)\n",
    "\n",
    "# 矩阵乘法\n",
    "# （n,m）(m,p)=(n,p) 相乘相加  \n",
    "data5=data1.matmul(data2)\n",
    "data6=data1@data2\n",
    "print(data5,data6)"
   ],
   "id": "63020a3e35a274af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5, 12],\n",
      "        [21, 32]]) tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "tensor([[19, 22],\n",
      "        [43, 50]]) tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  张量数学计算",
   "id": "6ac2602a770538ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:07:12.648984Z",
     "start_time": "2025-01-20T09:07:12.619901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data=torch.randint(0,10,[2,3],dtype=torch.float64)\n",
    "print(data)\n",
    "print(data.mean())\n",
    "print(data.mean(dim=0))  # dim==0是最外层括号\n",
    "print(data.mean(dim=1))\n",
    "\n",
    "print(data.sum())\n",
    "print(data.sum(dim=0))\n",
    "print(data.sum(dim=1))\n",
    "\n",
    "print(torch.pow(data,2))    # 幂函数\n",
    "\n",
    "print(torch.sqrt(data))     # 平方根幂函数  等于pow(data,1/2)\n",
    "\n",
    "print(torch.log(data))      # 对数函数\n",
    "\n",
    "print(torch.exp(data))      # 指数函数"
   ],
   "id": "e0a9ebba3f140ed3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 5., 2.],\n",
      "        [5., 8., 8.]], dtype=torch.float64)\n",
      "tensor(5.1667, dtype=torch.float64)\n",
      "tensor([4.0000, 6.5000, 5.0000], dtype=torch.float64)\n",
      "tensor([3.3333, 7.0000], dtype=torch.float64)\n",
      "tensor(31., dtype=torch.float64)\n",
      "tensor([ 8., 13., 10.], dtype=torch.float64)\n",
      "tensor([10., 21.], dtype=torch.float64)\n",
      "tensor([[ 9., 25.,  4.],\n",
      "        [25., 64., 64.]], dtype=torch.float64)\n",
      "tensor([[1.7321, 2.2361, 1.4142],\n",
      "        [2.2361, 2.8284, 2.8284]], dtype=torch.float64)\n",
      "tensor([[1.0986, 1.6094, 0.6931],\n",
      "        [1.6094, 2.0794, 2.0794]], dtype=torch.float64)\n",
      "tensor([[  20.0855,  148.4132,    7.3891],\n",
      "        [ 148.4132, 2980.9580, 2980.9580]], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 214
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 张量的索引操作",
   "id": "b26057c0376a2bb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 注意区分列表的索引操作 和  tensor的索引操作\n",
    "# x=[[1,2,3],[4,5,6]]\n",
    "# print(x[1][0])\n",
    "# print(x[0])取第一行\n",
    "# 不能用 [:,0] 取第一列这是ndarray和tensor的用法；也不能用[:][0]，这个实际上也是取第一行\n",
    "# 只能用list(zip(*data))[0]或者[row[0] for row in data]"
   ],
   "id": "4af5573b613cc253"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T19:34:18.053871Z",
     "start_time": "2025-06-27T19:34:18.041871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 注意与列表的索引操作作区分\n",
    "# x=[[1,2,3],[4,5,6]]\n",
    "# print(x[1][0])\n",
    "import torch  # 列表索引 布尔索引 范围索引\n",
    "torch.manual_seed(100)\n",
    "data=torch.randint(0,10,[4,5])    # 两句话，布尔索引看作正常的索引参与广播机制。:是例外的广播机制，可能会导致升维度。\n",
    "print(data)\n",
    "# print(data[0])   # 取第一行\n",
    "# print(data[0][0]) # 取第一行的第一列\n",
    "# print(data[:,0])    # 取第一列\n",
    "# print(data[0,0])      # 取第一行的第一列\n",
    "# print(data[[0,1],[1,2]]) # 无需广播，自动组合成 [(0,1),(1,2)]\n",
    "# print(data[[0],[1]],[1,2])  # 需要广播 （2，1）和（2） 广播成（2，2）\n",
    "# 广播规则\n",
    "# 从右往左对齐；\n",
    "# 左侧不足补1；\n",
    "# 每个维度判断兼容性（相等 or 1）；\n",
    "print(data[[0,1],[[1],[2]]])\n",
    "#(2,),(2,1)\n",
    "# (2,),(2,2)   [0,1],[[1,1],[2,2]]\n",
    "# (2,2),(2,2)  [[0,1],[0,1]],[[1,1],[2,2]]\n",
    "# [[(0,1),(1,1)],[(0,2),(1,2)]]\n",
    "'''第一个维度的索引是 [0,1]\n",
    "第二个维度的索引是 [[1],[2]]\n",
    "这种索引方式会进行广播匹配，实际上相当于访问以下位置的元素：\n",
    "(0,1)\n",
    "(0,2)\n",
    "(1,1)\n",
    "(1,2)'''\n",
    "\n",
    "# print(data[:3,:2])\n",
    "print(torch.tensor([2,3]).size())\n",
    "print(data[[2,2],[[1,2],[3,4]]])   # (4,2,2)\n",
    "print(data[[False,True,True,False],[[1,2],[3,4]]])\n",
    "\n",
    "#范围索引不广播\n",
    "print(data[:2,[[1],[2]]])  # :的包容          # （4，5）+（2，1）==（4，2，1）   (2,5)+(2,1)==(2,2,1)\n",
    "print(data[data[:,2]>5,[[1],[2]]])  \n",
    "print(data[[[1],[2]],:])   # 他的广播机制可能会导致升维度 （2，1）+（4，5）==（2，1，5）"
   ],
   "id": "d999d02508283046",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 5, 9, 5],\n",
      "        [1, 2, 8, 8, 0],\n",
      "        [4, 3, 8, 8, 4],\n",
      "        [4, 1, 0, 4, 3]])\n",
      "tensor([[2, 2],\n",
      "        [5, 8]])\n",
      "torch.Size([2])\n",
      "tensor([[3, 8],\n",
      "        [8, 4]])\n",
      "tensor([[2, 8],\n",
      "        [8, 4]])\n",
      "tensor([[[2],\n",
      "         [5]],\n",
      "\n",
      "        [[2],\n",
      "         [8]]])\n",
      "tensor([[2, 3],\n",
      "        [8, 8]])\n",
      "tensor([[[1, 2, 8, 8, 0]],\n",
      "\n",
      "        [[4, 3, 8, 8, 4]]])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T19:49:52.283853Z",
     "start_time": "2025-06-27T19:49:52.280239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 布尔索引 保存【】，参与广播机制，没有方括号代表降维，该位置没有方括号。\n",
    "print(data[data[:,2]>5])  # \n",
    "# （4，5）=（2，5）\n",
    "\n",
    "print(data[:,data[0]>5])   # （4，5）+（2）=（4，2）\n",
    "print(data[:,data[1]>5])   # （4，5）+（2）=（4，2）\n"
   ],
   "id": "996a052a9fe45bc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 8, 8, 0],\n",
      "        [4, 3, 8, 8, 4]])\n",
      "tensor([[9],\n",
      "        [8],\n",
      "        [8],\n",
      "        [4]])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:32:17.661092Z",
     "start_time": "2025-01-20T09:32:17.650954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 布尔索引\n",
    "data=torch.randint(0,10,[3,4,5])\n",
    "print(data)\n",
    "print(data[0,:,:]) # "
   ],
   "id": "c573c615517f3df8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 1, 4, 0],\n",
      "         [2, 4, 1, 5, 3],\n",
      "         [2, 5, 9, 6, 2],\n",
      "         [6, 5, 1, 2, 5]],\n",
      "\n",
      "        [[4, 9, 8, 0, 0],\n",
      "         [2, 5, 2, 1, 1],\n",
      "         [1, 3, 3, 5, 7],\n",
      "         [8, 9, 5, 3, 0]],\n",
      "\n",
      "        [[3, 5, 7, 7, 2],\n",
      "         [5, 8, 5, 2, 3],\n",
      "         [1, 9, 3, 3, 2],\n",
      "         [2, 4, 8, 0, 0]]])\n",
      "tensor([[0, 1, 1, 4, 0],\n",
      "        [2, 4, 1, 5, 3],\n",
      "        [2, 5, 9, 6, 2],\n",
      "        [6, 5, 1, 2, 5]])\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 张量形状操作",
   "id": "ceb820b5461b8aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T08:32:53.461096Z",
     "start_time": "2025-01-23T08:32:53.438347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.reshape()   # 元素重排列\n",
    "# tensor.view(shape)  # 操作的tensor必须再连续内存空间内\n",
    "# tensor.contiguous()  # 将tensor放在连续内存空间内\n",
    "# torch.stack(tensors,dim=1) # 将相同形状的tensor堆叠在一起，维度升高\n",
    "# torch.squeeze(input)    # 降低维度，减少括号\n",
    "# torch.unsqueeze(input)  # 生维度，增加括号\n",
    "# torch.transpose(input,dim1,dim2)  # 维度变换，元素相对位置不变，维度优先级的问题\n",
    "# torch.permute(input,dims)  # 形状变换，可指定多个维度\n",
    "\n",
    "# transpose和permute仅改变形状和步幅，不复制数据，步幅度指的是从逻辑索引到物理内存位置的映射规则。\n",
    "'''视图（View）特性：\n",
    "transpose和permute返回的是原张量的视图，仅修改元数据（形状、步幅），不复制数据。\n",
    "步幅（Stride）调整：\n",
    "例如，原张量形状为(2, 3)，步幅为(3, 1)，转置后形状为(3, 2)，步幅变为(1, 3)。元素通过新步幅访问，但内存位置未变。\n",
    "4. 注意事项\n",
    "连续性（Contiguity）：\n",
    "transpose和permute可能导致张量不连续（如转置后的矩阵）。若需要连续张量（如某些运算要求），需显式调用.contiguous()，此时会触发数据复制，物理存储改变。'''\n",
    "\n",
    "import torch\n",
    "x=torch.arange(1.,8.)\n",
    "print(x)\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "# reshape扩为度\n",
    "x_reshaped=x.reshape(1,7)\n",
    "print(x_reshaped)\n",
    "\n",
    "data=torch.tensor([[10,20,30],[40,50,60]])\n",
    "print(data.reshape(1,6))\n",
    "print(data.reshape(3,2))\n",
    "\n",
    "print(data.view(3,2))\n",
    "print(data.is_contiguous())\n",
    "data=data.view(3,2)\n",
    "print(data.is_contiguous())"
   ],
   "id": "65d86e26f775e3ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5., 6., 7.])\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "tensor([[1., 2., 3., 4., 5., 6., 7.]])\n",
      "tensor([[10, 20, 30, 40, 50, 60]])\n",
      "tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n",
      "tensor([[10, 20],\n",
      "        [30, 40],\n",
      "        [50, 60]])\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T08:39:19.641035Z",
     "start_time": "2025-01-23T08:39:19.625522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.arange(1,8)\n",
    "x_stacked=torch.stack([x,x,x],dim=0)\n",
    "print(x_stacked)"
   ],
   "id": "94b9477b20575b8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5, 6, 7],\n",
      "        [1, 2, 3, 4, 5, 6, 7],\n",
      "        [1, 2, 3, 4, 5, 6, 7]])\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#squeeze unsqueeze\n",
   "id": "2d89757391d8ee99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# transpose permute  交换维度，导致不连续，数据相对位置不变\n",
    "\n",
    "\n",
    "tensor.permute()\n",
    "data.permute"
   ],
   "id": "cb562b18164a56c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 拼接操作",
   "id": "ee1c8770ed364f61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.cat()",
   "id": "19ff229284bc4505"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 自动微分模块",
   "id": "5ca4583493ee9e8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 机器学习的监督学习\n",
    "# 1.准备数据：特征和目标 ，2.设计模型，损失函数和优化算法。\n",
    "# 2.获得模型\n",
    "\n",
    "# 机器学习的线性回归\n",
    "# data：一组 x,y ,向量x=[1,x1,x2,x3]和标量y  ps:线性回归只有一个输出，类似于神经网络中的一个神经元\n",
    "# model h(x)=θT⋅x    loss= MSE()= 1/m⋅(y^-y)²  优化方法：梯度下降\n",
    "\n",
    "# 线性回归到神经网络\n",
    "# 线性回归模型公式上是多元一次方程，几何上是拟合直线，平面，超级平面。\n",
    "# 如果为多元一次方程组引入激活函数，也就是引入非线性，那么这样的模型可以拟合曲线，曲面和超曲面。而这，就是一个神经元。\n",
    "# 如果我们将神经元模型叠加，就可以实现曲面叠加，形成网络层，可以拟合更复杂的情况和关系。"
   ],
   "id": "1637191940fdadbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 基本概念：\n",
    "# 梯度下降是一种最优化算法，通过沿着函数的梯度负方向迭代来寻找函数的局部最小值。\n",
    "\n",
    "# 梯度表示函数在某一点的最陡峭方向\n",
    "# 梯度的反方向指向函数值下降最快的方向\n",
    "# 通过沿着梯度的反方向迭代移动，最终到达局部最小值\n",
    "\n",
    "\n",
    "# 梯度下降算法是一种优化目标函数的迭代方法，通过不断调整参数值，使得目标函数值逐步减小。最终找到最小值或接近最小值。\n",
    "\n",
    "# 1.目标函数：\n",
    "# 梯度下降的目标是最小化一个函数 J(θ)，其中 θ 是模型的参数（如权重）。\n",
    "# 2.梯度：\n",
    "# 梯度是目标函数对参数的偏导数向量，表示函数在参数空间中的变化方向。梯度 ∇J(θ) 指向函数值增加最快的方向。\n",
    "# 3.更新规则：梯度下降通过以下公式更新参数：\n",
    "# θ = θ - α * ∇J(θ)\n",
    "# 其中，α 是学习率，控制每次更新的步长。\n",
    "# 4.迭代过程：\n",
    "# 从初始参数 θ0开始，反复计算梯度并更新参数，直到满足停止条件（如梯度接近零或达到最大迭代次数）。\n",
    "\n",
    "# 关键点\n",
    "# 学习率：α 的选择至关重要：过大可能导致震荡或发散。过小则收敛速度慢。\n",
    "# \n",
    "# 局部最优与全局最优：\n",
    "# 梯度下降可能陷入局部最优，尤其是在非凸函数中。初始参数和优化策略会影响结果。\n",
    "# \n",
    "# 批量梯度下降（BGD）：\n",
    "# 每次迭代使用整个数据集计算梯度，计算开销大但稳定。\n",
    "# \n",
    "# 随机梯度下降（SGD）：\n",
    "# 每次迭代随机选择一个样本计算梯度，计算效率高但波动较大。\n",
    "# \n",
    "# 小批量梯度下降（Mini-batch GD）：\n",
    "# 折中方案，每次使用一个小批量样本计算梯度，兼顾效率和稳定性。"
   ],
   "id": "cd21ca336b3ad07f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 在线性回归中，目标函数（损失函数）是最小化均方误差（MSE）：\n",
    "# \n",
    "# J(θ)=1/2m*(m∑i=1)(hθ(x(i))−y(i))²\n",
    "# \n",
    "# hθ(x)=θT*x 是预测函数。\n",
    "# \n",
    "# m m 是样本数量。\n",
    "# \n",
    "# 系数 1/2是为了简化梯度计算而引入的。\n",
    "\n",
    "# θ j=θ j−α⋅ (1/m)⋅   (i=1∑m)(h θ(x (i))−y (i))⋅x j(i)\n",
    "\n",
    "\n",
    "# α 是学习率。\n",
    "# \n",
    "# hθ(x(i))=θTx(i)是预测值。\n",
    "# \n",
    "# y(i)是真实值。\n",
    "# \n",
    "# x j(i)  是第 i个样本的第 j 个特征。                             \n",
    "\n",
    "\n"
   ],
   "id": "6dc880fe9e04183d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# θ 为参数，α 为学习率，∇J(θ) 为损失函数对参数的梯度\n",
    "# θ = θ - α * ∇J(θ)\n",
    "\n",
    "# 对于参数来说"
   ],
   "id": "e29d480c5853f528"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e9acb739460a9db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T07:26:19.683540Z",
     "start_time": "2025-02-05T07:26:19.600736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y * 3\n",
    "\n",
    "# 计算图会随着运算的执行即时构建\n",
    "z.backward()\n",
    "print(x.grad)  # 输出 tensor([6.])"
   ],
   "id": "6d952bcdc120ee57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T07:26:46.737544Z",
     "start_time": "2025-02-05T07:26:46.722282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代码执行到哪里，计算图就构建到哪里\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "if x > 1:\n",
    "    y = x * 2\n",
    "else:\n",
    "    y = x + 2\n",
    "# 计算图会根据实际执行的分支构建"
   ],
   "id": "d25fe8835e90df",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f6eebed6700a187"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T09:03:21.526503Z",
     "start_time": "2025-01-24T09:03:20.714637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.tensor(5.)\n",
    "y=torch.tensor(0.)\n",
    "print(x,x.dtype,y)\n",
    "w=torch.tensor(1.,requires_grad=True)\n",
    "b=torch.tensor(3.,requires_grad=True)\n",
    "\n",
    "z=x*w+b\n",
    "loss=torch.nn.MSELoss()\n",
    "\n",
    "loss=loss(z,y)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "id": "ae14089386ea3f08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.) torch.float32 tensor(0.)\n",
      "tensor(64., grad_fn=<MseLossBackward0>)\n",
      "tensor(80.)\n",
      "tensor(16.)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T08:43:13.531824Z",
     "start_time": "2025-02-05T08:43:13.506973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device=torch.device('cuda')\n",
    "x=torch.ones(2,5)\n",
    "y=torch.zeros(2,3)\n",
    "x.to(device)\n",
    "y.to(device)\n",
    "print(x,y)\n",
    "w=torch.randn(5,3,requires_grad=True)\n",
    "b=torch.randn(3,requires_grad=True)\n",
    "w.to(device)\n",
    "b.to(device)\n",
    "z=x@w+b\n",
    "\n",
    "loss=torch.nn.MSELoss()\n",
    "\n",
    "loss=loss(z,y)\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "id": "3a16a4fb003c207c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1.2742, 0.6795, 0.9647],\n",
      "        [1.2742, 0.6795, 0.9647],\n",
      "        [1.2742, 0.6795, 0.9647],\n",
      "        [1.2742, 0.6795, 0.9647],\n",
      "        [1.2742, 0.6795, 0.9647]])\n",
      "tensor([1.2742, 0.6795, 0.9647])\n"
     ]
    }
   ],
   "execution_count": 79
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
