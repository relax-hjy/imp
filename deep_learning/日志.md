
1.设置可见显卡
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"  # 必须在导入torch前设置！

torch.cuda.set_device(0)  # 使用的索引是映射过的

torch.cuda.current_device() # 查看当前的设备

2.使用多卡


3.AutoModelForCausalLM 加载因果性模型
device_map="auto"：多设备分布式加载。

torch_dtype=torch.float16

自定义模型：trust_remote_code=True（加载Hugging Face Hub自定义代码）

AutoModel 更灵活但需自定义输出层，而 AutoModelForCausalLM 开箱即用生成任务

无预配置头部	内置语言建模头部（lm_head）



0814

安装英伟达驱动 可以去官方文档看

instruct微调  chat微调  激活部分参数可能是moe架构


llama f  是微调模型的包

vllm能加速推理   tensor-rt

模型加载以fp16是两倍显存，fp324倍显存

